{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "Harsh-Agarwal_7024725_Zurana-Mehrin-Ruhi_7023892.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E924Sfqeqa9v"
      },
      "source": [
        "# NNTI 21/22: Multiple Linear Regression "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge4yiMJxqa9x"
      },
      "source": [
        "## Deadline: 03.12 December 2021, 23:59"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8ME3p3Cqa9y"
      },
      "source": [
        "**Important:** For all implementations in this assignment, make sure to use PyTorch whenever possible. Most computations on vectors and matrices can be implemented very efficiently using the PyTorch API. There is no need for looping over vectors etc using `for` loops. As a simple example, in order to compute the mean of a vector, just use `torch.mean()`. If you are not familiar with PyTorch please consult the PyTorch tutorials online. Further, in case of any doubts, the Piazza forum is the best place to ask questions and clarifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaOm_XRmqa9y"
      },
      "source": [
        "import itertools\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUX4eMn8qa9z"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "3rXYSkY-qa90"
      },
      "source": [
        "## 4.3. Multiple Linear Regression (8.0 points)\n",
        "\n",
        "In this exercise you will learn about [*multiple linear regression*](https://en.wikipedia.org/wiki/Linear_regression#Simple_and_multiple_linear_regression) while also experimenting with hyperparameter tuning. Performing regression on one independent (or explanatory) variable and a scalar dependent variable is called **simple linear regression**.\n",
        "But, when there are more than one explanatory variable (i.e. $x^{(1)}, x^{(2)}, ...,x^{(k)}$), and a single scalar dependent variable (*y*), then it's called **multiple linear regression**. (Do not confuse this with *multivariate linear regression* where we predict more than one (correlated) dependent variable.)\n",
        "\n",
        "Here, you will be implementing a **multiple linear regression** model in Python/PyTorch using the [*vanilla Gradient Descent*](https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants) algorithm. Particularly, we will be using a variant of the **stochastic gradient descent** (*SGD*) where one performs the update step using a small set of training samples of size *batch_size* which we will set to 64, i.e. we go through the training samples, sampling 64 at a time, and perform gradient descent. Such a procedure is sometimes called as **mini-batch gradient descent** in the deep neural networks community.\n",
        "\n",
        "Going through all the training samples *once* is called an **epoch**. Ideally, the training procedure has to go through multiple epochs over the training samples, each time shuffling it, until a convergence criterion has been satisfied. Here, we will set a *tolerance value* for the difference in error (i.e. change in Mean Squared Error (MSE) values between subsequent epochs) that we will accept. Once this difference falls below the *tolerance value*, we terminate our training phase and return the latest parameters. \n",
        "\n",
        "We repeat the above training procedure for all possible hyperparameter combinations in order to find the best parameters (i.e. weights) for our model. For this so called *hyperparameter tuning* we will be using the validation data. \n",
        "\n",
        "As a next step, we will combine training data and validation data and make it as our *new training data*. We keep the test data as it is, untouched throughout our experiments. Using the hyperparameter combination (for the least MSE) that we found above, we train the model *again* with the *new training data* and obtain the parameter (*i.e. weight vector*) after convergence according to our *tolerance value*.\n",
        "\n",
        "Phew! That will be our much desired *weight vector*. This is then used on the *test data*, which has not been seen by our algorithm so far, to make a prediction. The resulting MSE value will be the so-called [*generalization error*](https://en.wikipedia.org/wiki/Generalization_error). It is this *generalization error* that we want it to be as low as possible for some *unseen data* (implies that we can achieve higher accuracy)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKJVpa1Nqa91"
      },
      "source": [
        "#### 4.3.1 Dataset\n",
        "For our task, we will be using the *Wine Quality* dataset and predict the quality of white wine based on 11 features such as acidity, citric acid content, residual sugar etc. . You can take a glance of the data using functions like *data.head()*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "wVef9B8_qa91",
        "outputId": "a5e5e301-7f10-4575-9c1e-c5a650a21d4e"
      },
      "source": [
        "# get data\n",
        "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n",
        "data = pd.read_csv(data_url, sep=';')\n",
        "\n",
        "# inspect data\n",
        "display(data.head())\n",
        "print(data.shape)\n",
        "\n",
        "# Get data as NumPy array\n",
        "data_np = data.values\n",
        "\n",
        "# Convert the data to Torch tensor\n",
        "data_tensor = torch.tensor(data_np, dtype=torch.float64)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>20.7</td>\n",
              "      <td>0.045</td>\n",
              "      <td>45.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>1.0010</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.45</td>\n",
              "      <td>8.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.049</td>\n",
              "      <td>14.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.9940</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.1</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.40</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0.050</td>\n",
              "      <td>30.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.9951</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.44</td>\n",
              "      <td>10.1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
              "0            7.0              0.27         0.36  ...       0.45      8.8        6\n",
              "1            6.3              0.30         0.34  ...       0.49      9.5        6\n",
              "2            8.1              0.28         0.40  ...       0.44     10.1        6\n",
              "3            7.2              0.23         0.32  ...       0.40      9.9        6\n",
              "4            7.2              0.23         0.32  ...       0.40      9.9        6\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4898, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "6D34B-9lqa92"
      },
      "source": [
        "#### 4.3.2. Loss function\n",
        "We will use a *regularized* form of the MSE loss function. In matrix-vector format it can be written as follows:\n",
        "\n",
        "\\begin{equation*}\n",
        "    J(\\textbf{w}) = \\frac{1}{2} \\Vert{X\\textbf{w}-\\textbf{y}}\\Vert^{2} + \\frac{\\lambda}{2}\\Vert{\\textbf{w}}\\Vert^{2}\n",
        "\\end{equation*}\n",
        "\n",
        "It's important to note that, in the above equation, $X$, called [**design matrix**](https://en.wikipedia.org/wiki/Design_matrix#Definition), consists of data points in our dataset. Each row corresponds to a data point whereas each column represents a feature. Therefore, the dimension of $X$ is *(number of data points, number of features)*. $X$ can be also thought as of the vertical concatenation of data points of shape *(batch_size, num_features)*. To make things easier and computationally efficient, you can add the *bias* term as the first column of $X$. Take care to have the *weight* vector $\\textbf{w}$ with matching dimensions. <br > (Hint: see [Design_matrix#Multiple_regression](https://en.wikipedia.org/wiki/Design_matrix#Multiple_regression) for how $X$ with 2 features looks like for a $1^{st}$ degree polynomial.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emCS9mTVqa93"
      },
      "source": [
        "**Task 1:** (0.25 point) <br >\n",
        "Derive the gradient (w.r.t $\\textbf{w}$) for the regularized objective given in 4.3.2. \\[**Hint**: You can use your results from 3.2(a). However, make sure to adapt the result to the above objective\\]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihuwIOSyqa93"
      },
      "source": [
        "*Answer:* \n",
        "Answered in *pdf*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMm7KUjkqa93"
      },
      "source": [
        "---\n",
        "**Points:** $0.0$ of $0.25$\n",
        "\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "655MPEZHqa94"
      },
      "source": [
        "#### 4.3.3. Matrix-vector format for higher order polynomial\n",
        "\n",
        "Written in matrix form, a linear regression model for second-order would look like: <br />\n",
        "$$\\hat{\\textbf{y}} = X\\textbf{w}_{1} + X^{2}\\textbf{w}_{2} + \\textbf{b}$$\n",
        "\n",
        "where $X^{2}$ is the element-wise squaring (i.e., Hadamard product of X with itself) of the original design matrix $X$, $\\textbf{w}_1$ and $\\textbf{w}_2$ are the *weight* vectors, and **b** is the *bias* vector.\n",
        "\n",
        "**Task 2:** (0.25 point) <br >\n",
        "Write down the matrix-vector format for an $8^{th}$ order linear regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBkwfyvVqa94"
      },
      "source": [
        "*Answer:* \n",
        "\n",
        "$$\\hat{\\textbf{y}} = X\\textbf{w}_{1} + X^{2}\\textbf{w}_{2} + X^{3}\\textbf{w}_{3} + X^{4}\\textbf{w}_{4} + X^{5}\\textbf{w}_{5} + X^{6}\\textbf{w}_{6} + X^{7}\\textbf{w}_{7} + X^{8}\\textbf{w}_{8} +\\textbf{b}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfU7Pzjrqa94"
      },
      "source": [
        "---\n",
        "**Points:** $0.0$ of $0.25$\n",
        "\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "WZ5gnbPDqa95"
      },
      "source": [
        "#### 4.3.4. Hyperparameters\n",
        "Next, we will experiment with three hyperparameters for developing our model:\n",
        "\n",
        "i) regularization parameter $\\lambda$ <br />\n",
        "ii) learning rate $\\epsilon$ <br />\n",
        "iii) order of polynomial *p*\n",
        "\n",
        "And do a grid search over the values that these hyperparameters can take in order to select the best combination (i.e. the one that will achieve the lowest *test* error on our data). This approach is called **hyperparameter optimization or tuning**. For convenience and computational reasons, we will experiment with only three values for each of the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "ovslzLmSqa95"
      },
      "source": [
        "# Fix possible hyperparameters\n",
        "polynomial_orders = [1, 5, 9]\n",
        "learning_rates = [1e-5, 1e-6, 1e-8]\n",
        "lambdas = [0.1, 0.5, 0.8]\n",
        "\n",
        "# Fix batch size\n",
        "batch_size = 64"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "4UOy-SSdqa96"
      },
      "source": [
        "#### 4.3.5. Normalization\n",
        "First of all, inspect the data, and understand its structure and features. Ideally, before starting to train our learning algorithm, we would want the data to be normalized. Here, we normalize the data (i.e. normalize each column) using the following formula:\n",
        "\n",
        "\\begin{equation*}\n",
        "  norm\\_x_i = \\frac{x_i - min(x)}{max(x) - min(x)}\n",
        "\\end{equation*}\n",
        "where $x_i$ is the $i^{th}$ sample in feature $x$.\n",
        "\n",
        "**Task 3:** (0.25 point) <br > \n",
        "Complete the following function which performs normalization (i.e. normalizes the columns of $X$). Use only PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqHHTvMwqa96"
      },
      "source": [
        "def data_normalization(data):\n",
        "    # TODO: implement\n",
        "    data = (data - torch.min(data, 0)[0]) / (torch.max(data, 0)[0] - torch.min(data, 0)[0])\n",
        "    return data\n",
        "\n",
        "# Perform data normalization\n",
        "norm_data = data_tensor\n",
        "norm_data[:, :-1] = data_normalization(data_tensor[:, :-1])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEoZ1a22qa96"
      },
      "source": [
        "---\n",
        "**Points:** $0.0$ of $0.25$\n",
        "\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhEPY9wTqa97"
      },
      "source": [
        "#### 4.3.6. Data Splitting and Shuffling\n",
        "Typically, we need to divide our data into 3 splits \\[ train (80%), validation (10%), and test (10%)\\] for experimentation purposes. And shuffle the training data during every epoch.\n",
        "\n",
        "**Task 4**: 0.25 points <br >\n",
        "Implement the following function `split_data()`. You can either implement it manually using `torch` or use sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "dVHjIcNuqa97"
      },
      "source": [
        "# Split the data into training, validation, and test sets\n",
        "def split_data(data, n_train=3898, n_val=500, n_test=500):\n",
        "    # TODO: implement\n",
        "    \n",
        "    train_data = data[: n_train]\n",
        "    val_data = data[n_train : n_train+n_val]\n",
        "    test_data = data[-n_val :]\n",
        "    \n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "\n",
        "# Shuffle only the training data along axis 0\n",
        "def shuffle_train_data(X_train, Y_train):\n",
        "    \"\"\"called after each epoch\"\"\"\n",
        "    # shuffling of data along axis 0\n",
        "    rand_idx = torch.randperm(X_train.size()[0]) \n",
        "    \n",
        "    X_train = X_train[rand_idx]\n",
        "    Y_train = Y_train[rand_idx]\n",
        "    \n",
        "    return X_train, Y_train"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "464JG8gH_l0B"
      },
      "source": [
        "train_data, val_data, test_data = split_data(norm_data)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roDvq-R1qa97"
      },
      "source": [
        "---\n",
        "**Points:** $0.0$ of $0.25$\n",
        "\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHKkeb6pqa98"
      },
      "source": [
        "#### 4.3.7. Implementation of required functions\n",
        "\n",
        "**Task 5:** (0.5 point) <br >\n",
        "Complete the following function which computes the MSE value. You can ignore the regularization term and also the constant $\\frac{1}{2}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "i_fjh-_eqa98"
      },
      "source": [
        "# Compute Mean Squared Error \n",
        "def compute_mse(prediction, ground_truth):\n",
        "    '''\n",
        "    :param prediction: a nx1 vector represents the prediciton of your model\n",
        "    :param ground_truth: a nx1 vector represents the ground_truth\n",
        "    :return: MSE loss\n",
        "    '''\n",
        "    # TODO: implement\n",
        "\n",
        "    mse_loss = torch.sum(((prediction - ground_truth)**2) / len(prediction))\n",
        "    \n",
        "    return mse_loss"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK8wdW8yqa98"
      },
      "source": [
        "---\n",
        "**Points:** $0.0$ of $0.5$\n",
        "\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXhcxRQRqa99"
      },
      "source": [
        "**Task 6:** (0.5 point) <br >\n",
        "Implement the function which computes the prediction of your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "53O-_SoDqa99"
      },
      "source": [
        "def get_prediction(X, W):\n",
        "    '''\n",
        "    Given a design matrix X (could be a batch) and parameters W, calculate the prediction Yhat.\n",
        "    :param X: desgin matrix X of dimension nxk, where n is the number of data points (in the batch).\n",
        "    :param W: parameters\n",
        "    :return Yhat: the predictions\n",
        "    '''\n",
        "    # TODO: implement\n",
        "    y_hat = torch.matmul(X, W)\n",
        "    \n",
        "    return y_hat"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0EHNjbYqa99"
      },
      "source": [
        "---\n",
        "**Points:** $0.0$ of $0.5$\n",
        "\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98fEtB6yqa99"
      },
      "source": [
        "**Task 7:** (0.25 point) <br >\n",
        "Implement the function which computes the gradient of your loss function. That is, implement the gradient arrived at in Task 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "zqaKRcS_qa9-"
      },
      "source": [
        "def compute_gradient(X, Y, Yhat, W, lambda_):\n",
        "    '''\n",
        "    :param X: designmatrix X\n",
        "    :param Y: ground truth labels correspoinding to X\n",
        "    :param Yhat: predicted labels\n",
        "    :param W: parameters\n",
        "    :param lambda_: coefficient for the regularizer\n",
        "    :return: gradient w.r.t W\n",
        "    '''\n",
        "    # TODO: implement\n",
        "    gradient = torch.matmul(X.T, (Yhat - Y)) + lambda_ * W\n",
        "    \n",
        "    return gradient"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-dDx7Lbqa9-"
      },
      "source": [
        "---\n",
        "**Points:** $0.0$ of $0.25$\n",
        "\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-WqDj98qa9-"
      },
      "source": [
        "**Task 8:** (0.5 point) <br >\n",
        "Implement the function which performs a single update step of mini-batch GD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "QguqH4vdqa9-"
      },
      "source": [
        "# Hint: avoid in-place modification\n",
        "def sgd(gradient, lr, cur_W):\n",
        "    '''\n",
        "    :param gradient: gradient at cur_W\n",
        "    :param lr: learning rate\n",
        "    :param cur_W: current value of parameters\n",
        "    :return new_W: perform parameter update (using gradient descent) and return new_W\n",
        "    '''\n",
        "    # TODO: implement\n",
        "    new_w = cur_W - lr * gradient   \n",
        "    return new_w"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjHB8110qa9_"
      },
      "source": [
        "---\n",
        "**Points:** $0.0$ of $0.5$\n",
        "\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G70ZFtHgqa9_"
      },
      "source": [
        "**Task 9:** (0.5 point) <br >\n",
        "Complete the following function which reformats your data as a design matrix, and accordingly the weight vector, for a given polynomial order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "O_G49t9Vqa9_"
      },
      "source": [
        "# concatenate X acc. to order of polynomial; likewise do it for W\n",
        "# where X is design matrix, W is the corresponding weight vector\n",
        "# concatenate matrix X horizontally, concatenate W vertically. That is, after this transformation, X gets broader and W gets longer\n",
        "# e.g. [1 X X^2 X^3], [1 W1 W2 W3]   #the square brackets does not signify python lists\n",
        "def prepare_data_matrix(X, W, order):\n",
        "    # TODO: implement\n",
        "    X_data_matrix = torch.ones((X.shape[0], 1))\n",
        "    W_data_matrix = torch.ones((1, 1))\n",
        "    for i in range(order):\n",
        "      X_data_matrix = torch.cat((X_data_matrix, torch.pow(X, i+1)), axis=1)\n",
        "      W_data_matrix = torch.cat((W_data_matrix, torch.pow(W, i+1)))\n",
        "    return X_data_matrix, W_data_matrix"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWiCaYE7qa-A"
      },
      "source": [
        "---\n",
        "**Points:** $0.0$ of $0.5$\n",
        "\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br3lWJcBqa-A"
      },
      "source": [
        "#### 4.3.8. Training\n",
        "\n",
        "**Task 10:** (3 points) <br >\n",
        "Complete the code in the following cell such that it performs **mini-batch gradient descent** on the training data for all possible hyperparameter combinations.\n",
        "\n",
        "Note: You can also define a function, named appropriately (e.g. `train()`), which performs training. And, take care to do correct bookkeeping of hyperparameter combinations, weight vectors, and the MSE values in your function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "BglK9HyPqa-A"
      },
      "source": [
        "def train(X_train, Y_train, W_init, polynomial_orders, learning_rates, lambdas):\n",
        "    # TODO: implement\n",
        "    \n",
        "    loss_diff_tolerance = 0.0001\n",
        "    max_epochs = 500\n",
        "\n",
        "    train_params = {} # Dictionary storing train_loss per epoch and weights for every [order, lambda, lr] combination \n",
        "    \n",
        "    for order in polynomial_orders:\n",
        "      X_train_order, W_init_order = prepare_data_matrix(X_train, W_init, order)\n",
        "      train_params[order] = {}\n",
        "\n",
        "      for lambda_ in lambdas:\n",
        "        train_params[order][lambda_] = {}\n",
        "      \n",
        "        for lr in learning_rates:\n",
        "          train_params[order][lambda_][lr] = {}\n",
        "          train_params[order][lambda_][lr]['loss'] = {}\n",
        "          cur_w  = W_init_order\n",
        "          loss_f = 9999999  #initalizing loss with infinity\n",
        "      \n",
        "          for epoch in range(max_epochs):\n",
        "            X_train_order, Y_train = shuffle_train_data(X_train_order, Y_train)\n",
        "      \n",
        "            for start_idx in range(0, len(X_train_order), batch_size):\n",
        "              idxs  = (np.arange(start_idx, min(len(X_train_order), start_idx + batch_size)))\n",
        "              preds = get_prediction(X_train_order[idxs], cur_w) # prediction with initial weights \n",
        "              grad  = compute_gradient(X_train_order[idxs], Y_train[idxs], preds, cur_w, lambda_)\n",
        "              cur_w = sgd(grad, lr, cur_w)\n",
        "\n",
        "            preds  = get_prediction(X_train_order, cur_w)\n",
        "            loss_i = loss_f\n",
        "            loss_f = compute_mse(preds, Y_train)\n",
        "            train_params[order][lambda_][lr]['loss'][epoch] = loss_f\n",
        "            if (torch.abs(loss_i - loss_f) < loss_diff_tolerance):\n",
        "              break\n",
        "\n",
        "          train_params[order][lambda_][lr]['w'] = cur_w\n",
        "\n",
        "    return train_params"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msEUslLTqa-A"
      },
      "source": [
        "---\n",
        "**Points:** $0.0$ of $3$\n",
        "\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFlYgZ2aqa-B"
      },
      "source": [
        "**Task 11:** (0.25 point) <br >\n",
        "Complete the following function which selects the best hyperparameter combination given a set of weights (i.e. the one that gives lowest MSE on **validation data**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "wHRmEjKuqa-B"
      },
      "source": [
        "# Select hparams of minimum MSE on Validation data\n",
        "def select_best_hparams(X_val, Y_val, train_params):\n",
        "    # TODO: Implement\n",
        "    best_mse = 9999\n",
        "    for order in train_params.keys(): #orders\n",
        "      for lambda_ in train_params[order].keys():\n",
        "        for lr in train_params[order][lambda_].keys():\n",
        "          cur_w = train_params[order][lambda_][lr]['w']\n",
        "          X_val_order, _ = prepare_data_matrix(X_val, cur_w, order)\n",
        "          preds = get_prediction(X_val_order.float(), cur_w.float())\n",
        "          if best_mse > compute_mse(preds, Y_val):\n",
        "            best_mse = compute_mse(preds, Y_val)\n",
        "            order_best, lambda_best, lr_best = order, lambda_, lr\n",
        "    return order_best, lambda_best, lr_best"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrtKpcfEqa-B"
      },
      "source": [
        "---\n",
        "**Points:** $0.0$ of $0.25$\n",
        "\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhVJz-M2qa-B"
      },
      "source": [
        "**Task 12:** (0.25 point) <br >\n",
        "Train the model for all possible hyperparameter combinations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbH2UWcCqa-B"
      },
      "source": [
        "# Train the model with all possible hyperparameter combinations\n",
        "# W_init_train = torch.rand(train_data[:, :-1].shape[1], 1)*0.4 - 0.2 #[-0.2, 0.2]\n",
        "W_init_train = torch.zeros(train_data[:, :-1].shape[1], 1)\n",
        "X_train = train_data[:, :-1]\n",
        "Y_train = train_data[:, -1:]\n",
        "train_params_results = train(X_train.float(), Y_train.float(), W_init_train.float(), polynomial_orders, learning_rates, lambdas)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjhF_WqOju4B"
      },
      "source": [
        "X_val = val_data[:, :-1]\n",
        "Y_val = val_data[:, -1:]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyEd9HJb06VM",
        "outputId": "20f6e0a0-38c0-41aa-9505-c5a843dd32bc"
      },
      "source": [
        "# Find best hyperparameter combination\n",
        "order_best, lambda_best, lr_best = select_best_hparams(X_val, Y_val, train_params_results)\n",
        "print(\"The best hyperparameters are: Order = {}, Lambda = {}, Learning rate = {}\".format(order_best, lambda_best, lr_best))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best hyperparameters are: Order = 1, Lambda = 0.1, Learning rate = 1e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbbEhpNzqa-C"
      },
      "source": [
        "---\n",
        "**Points:** $0.0$ of $0.25$\n",
        "\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7YwkkTZqa-C"
      },
      "source": [
        "#### 4.3.9. Re-Training on Train + Validation data\n",
        "**Task 13:** (0.5 point) <br >\n",
        "Now, we will concatenate the training and validation data and make it as the new training data.\n",
        "Complete the following which does re-training on the combined training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pThL2FyOqa-C"
      },
      "source": [
        "# Re-run the training on X_train + X_val combined\n",
        "# Re-run the training on X_train + X_val combined\n",
        "X_new_train = torch.concat((X_train, X_val))\n",
        "Y_new_train = torch.concat((Y_train, Y_val))\n",
        "W_init_train = torch.zeros(X_new_train.shape[1], 1)\n",
        "\n",
        "# TODO: implement\n",
        "train_params_results = train(X_new_train.float(), Y_new_train.float(), W_init_train.float(), [order_best], [lr_best], [lambda_best])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_Lnk6cRqa-C"
      },
      "source": [
        "---\n",
        "**Points:** $0.0$ of $0.5$\n",
        "\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z6noRmWqa-C"
      },
      "source": [
        "---\n",
        "**Visualizing MSE over epochs:** \n",
        "\n",
        "**Task 14**: 0.25 points <br>\n",
        "Plot the MSE values (y-axis) against epochs (x-axis) using matplotlib.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs7KYKIsqa-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "355c967d-9003-4507-cb83-f45921adb457"
      },
      "source": [
        "# Let's plot the convergence of MSE values using matplotlib, i.e. #epochs on X-axis and MSE values on Y-axis\n",
        "# TODO: implement\n",
        "fig, axes = plt.subplots()\n",
        "\n",
        "axes.plot(list(train_params_results[order_best][lambda_best][lr_best]['loss'].keys()), list(train_params_results[order_best][lambda_best][lr_best]['loss'].values()))\n",
        "axes.set_title('order = {}, lambda={}, lr={}'.format(order_best, lambda_best, lr_best))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'order = 1, lambda=0.1, lr=1e-05')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAaOklEQVR4nO3de5CldX3n8fen+/R9mrk2wwSGGUAiggroBHCNLt6RmHgtN4RNoEJBjGYXU2YNulnFMljGrURNqcSJsGgKiRIxshQr4gQ1qAX2IHLJODDgwABz6XHu9+nu7/7xPGf66TN979N9+nfO51V1as5z/z6/PvPpX/+e55yjiMDMzNLTVOsCzMxsahzgZmaJcoCbmSXKAW5mligHuJlZohzgZmaJcoAnTtItkv661nVMh6SNkt44A/u9WNJzk1j/Skn3V7uO2SIpJL2o1nXY7HGAW1VIWi1pvaRBSVfWup65RNKfS9oiaY+kmyW1jbJeq6R/yX+hhaSLZ7nUch2vk3SfpN2SNlZhf2+Q9EtJB/L9rigsu0XSEUn7Co/m6R6zUTjAE1LNF7akUrX2lfsF8H7goSrvN2mS3gJcB7wBWAGcDnxijE3uB/4rsKXKdUzmtbMfuBn4H1U47hLgDuB/AYuAXuAbFat9JiLmFR4D0z1uo3CA15ikl0j6gaRdkh6X9HuFZbdIulHS3ZL2A6+TdL6khyTtlfQNoL1if2+T9HC+v59Ienlh2UZJfynpEWB/NUM8Ir4YEWuAQ9PZj6QLJP00r3+zpC9Iai0sD0nvl/Rk3gaflHRGfq57JH2zuH6+zUclbc/P//LC/MWS7sy3exA4o2K7z0valC9fK+k1UzilK4CbIuLxiNgJfBK4cqQVI+JIRHwuIu4HphViI712JrptRDwYEf8EPD3Kvs+SdK+kHflfXe8dY3fvAh6PiNsj4hBwPXCupLMmcTo2Cgd4DUlqAf4v8D3gROC/AbdKenFhtT8AbgC6gQeBfwX+iaw3czvw7sL+zifrOf0JsBj4MnBnxZ/slwG/AyyIiP4RanokD8+RHl+q1rmPYQD4c2AJ8Cqynuv7K9Z5C/BK4CLgw8Bqsl7rcuClZOdYdlK+r5PJwnR1oX2/SPYLZxnwx/mj6GfAeWRt/XXgdkntAJL+YIx22iXp1Hwf55D9dVL2C2CppMWTbJepKL527pd03Vg1T2SHkrqAe8na40Tg94EvSTp7lE2GnX9E7AeeyueXvT//ZbBW0rsrd2BjiAg/avQAXkP2p3JTYd5twPX581uArxWWvRZ4AVBh3k+Av86f3wh8suIY64H/nD/fCPzxDJ/T/cCVk9xmI/DGUZZ9EPh2YTqAVxem1wJ/WZj+W+Bz+fOLgX6gq7D8m2R/zjcDR4GzCss+Bdw/Rp07gXMneW5PAZcUplvyc1g5znbPARdP8lgBvGik184Uf5ZvBDZWzPsvwL9XzPsy8PFR9nET8OmKeT8uv0aAV5B1NkrApcDe4s/Xj7Ef7oHX1m8AmyJisDDvGbLeYtmmivWfj/yVX1i/bAXwoYpe1fJ8u5H2N+dI+k1Jdym/6EcWqksqVttaeH5whOl5hemdkfX6yp4ha48estDYVLGsWMtfSFqXX8zbBcwfoZbx7ANOKEyXn++d5H6mYiZ+1iuACyteY5cDJ0k6tXgxMl+/8vzJp/cCRMRDEfHriOiPiLuBW8mGXWwCHOC19QKwXFLx53Aq8HxhuhjWm4GTJali/bJNwA0RsaDw6IyI20bZ33Hycfh9ozz+YXKnNyU3Ar8EzoyIE4CPAhp7kzEtzP/sLzuVrN37yHrnyyuWAZCPd38YeC+wMCIWALvLtUi6fIx22lcYQnkcOLdwjHOBrRHx62mc00QN+1nn1wJGrXmC+9wE/LDiNTYvIv40Ip6NwsXIfP1h55//LM7I549W83R+3g3FAV5bDwAHgA9LalF229jvAv88yvo/JQud/56v/y7ggsLyfwTeJ+lCZbok/Y6k7okWFBHnxPA7AoqP9422nbJb4NrJ/vO1SGov/2JSdj/2RD+3uBvYA+zLL3T96URrH8Mn8vpeA7wNuD2yOx3uAK6X1JmP4V5RUUc/WdCXJH2MQk8yIm4do53mRcSz+apfA66SdLakBcBfkQ1vjEhSW3mcHWjN27H8S+NKTeO2voj41Fg1F2poymtoySbVrqELw3cBvynpD/PXYIuk35L0klEO+23gpZLene/zY8AjEfHL/FjvkTQvP+abya5l3DnVc2w0DvAaiogjZIH9VmA78CXgj8ov7lHWfxfZXQw7yMYj7ygs7wWuBr5ANl67gVHueJgB3yMbvvhPZBcVD5KN2UPWy/3JBPfzF2QX3/aS/UKqvOVssraQtcULZH+ev6/Qvn9GNtyyhSxU/09hu3uA7wJPkA2tHGIKQxIR8V3gM8B9wLP5vj5eXp7/xXN5YZP1ZG13cl7DQbJhC8ja8ceTrWEKXpsf926yv0oOkv18iYi9wJvJLl6+QNZ2fwOMeG97RPSRXWi/gezncGG+bdm1ZH9x7gL+N3B1RPyg2idUrzR8ONWs+iR9hazXe0+ta0mZpO8B10bEulrXYnODA9zMLFEeQjEzS5QD3MwsUQ5wM7NEVfsDjca0ZMmSWLly5Wwe0swseWvXrt0eET2V82c1wFeuXElvb+9sHtLMLHmSnhlpvodQzMwS5QA3M0uUA9zMLFEOcDOzRDnAzcwS5QA3M0uUA9zMLFFJBPiadVv50g821LoMM7M5JYkA/+ETfaz+0YhfkG1m1rCSCPDW5iaO9A+Ov6KZWQNJI8BLTRx2gJuZDZNEgLeVmhkYDAYG/eUTZmZlSQR4aykr08MoZmZDHOBmZolKKsAPDwzUuBIzs7kjiQBva3YP3MysUhIBfqwH7gA3Mztm3ACXtFzSfZL+Q9Ljkq7N5y+SdK+kJ/N/F85UkW0eAzczO85EeuD9wIci4mzgIuADks4GrgPWRMSZwJp8ekb4IqaZ2fHGDfCI2BwRD+XP9wLrgJOBtwNfzVf7KvCOmSryWIAPOMDNzMomNQYuaSVwPvAAsDQiNueLtgBLR9nmGkm9knr7+vqmVGSrL2KamR1nwgEuaR7wLeCDEbGnuCwiAhjxbZIRsToiVkXEqp6enikV6SEUM7PjTSjAJbWQhfetEXFHPnurpGX58mXAtpkpsXgXiu8DNzMrm8hdKAJuAtZFxN8VFt0JXJE/vwL4TvXLy7SVmgHfRmhmVlSawDqvBv4QeFTSw/m8jwKfBr4p6SrgGeC9M1OibyM0MxvJuAEeEfcDGmXxG6pbzsh8F4qZ2fHSeCem70IxMztOGgHuIRQzs+M4wM3MEpVEgJeahOS7UMzMipIIcEm0lZp8EdPMrCCJAAd/M72ZWaV0ArzU7CEUM7OCZAK8reQeuJlZUTIB3uoxcDOzYdIJ8OYmDh/1h1mZmZUlE+BtLe6Bm5kVJRPgvgvFzGy4dALcFzHNzIZJK8A9hGJmdkw6Ae4hFDOzYdIJ8FKT38hjZlaQTIC3lZrdAzczK0gmwN0DNzMbLpkAz95K7zfymJmVJRPgvgvFzGy4dAI8vwslImpdipnZnJBOgJeaGAzoH3SAm5lBYgEO/l5MM7OyZAK8zQFuZjZMMgF+rAfuC5lmZkBKAd7sHriZWVE6AZ73wP1mHjOzTDIB7jFwM7PhkgnwoR64341pZgYJBXhbqRlwD9zMrCyZAPddKGZmw6UT4L4LxcxsmHQC3BcxzcyGSS/APYRiZgYkFODl2wgPH3WAm5lBQgHe3pLdhXLItxGamQEJBXhHHuAHjzjAzcwgoQA/1gP3EIqZGZBQgDc3idbmJg4edQ/czAwmEOCSbpa0TdJjhXnXS3pe0sP549KZLTPT3tLEIQe4mRkwsR74LcAlI8z/bESclz/urm5ZI+tobfYYuJlZbtwAj4gfATtmoZZxdbQ0+y4UM7PcdMbA/0zSI/kQy8LRVpJ0jaReSb19fX3TOFx2IdM9cDOzzFQD/EbgDOA8YDPwt6OtGBGrI2JVRKzq6emZ4uEy7S3NvohpZpabUoBHxNaIGIiIQeAfgQuqW9bIOlqafRHTzCw3pQCXtKww+U7gsdHWrabsLhTfB25mBlAabwVJtwEXA0skPQd8HLhY0nlAABuBP5nBGo/paPUQiplZ2bgBHhGXjTD7phmoZVy+iGlmNiSZd2JCNgbu78Q0M8skFeDugZuZDUkqwDvy2wgjotalmJnVXFoB3trMYPhbeczMILEA90fKmpkNSSzAs3L9Zh4zs8QC3N/KY2Y2JM0Adw/czCytAG9vLY+BO8DNzNIK8JJ74GZmZUkFeId74GZmx6QV4McuYvo2QjOzpALctxGamQ1JKsB9F4qZ2ZCkAtx3oZiZDUkqwP1GHjOzIUkFeEtzE81N4pA/E9zMLK0Ah/wjZX0XiplZegHe3uLvxTQzgwQDvKO1icMOcDOz9AK8veQeuJkZJBjgHa0OcDMzSDDA/cXGZmaZ5AK8o6WZQ/2+C8XMLLkAb29p4uCR/lqXYWZWc8kFeGdriQMeQjEzSy/A57WV2H/YPXAzs+QCvKutxD4HuJlZegE+r62ZowPBYX8eipk1uAQDvATA/sMOcDNrbMkFeFce4PsOeRjFzBpbcgFe7oF7HNzMGl16Ad6eD6H4XnAza3DJBXiXe+BmZkCCAT7PY+BmZkCCAd517C4UB7iZNbbkAtwXMc3MMskFeFdr9s30DnAza3TJBXipuYn2liYPoZhZwxs3wCXdLGmbpMcK8xZJulfSk/m/C2e2zOHmtbWwz+/ENLMGN5Ee+C3AJRXzrgPWRMSZwJp8etbMa2v2EIqZNbxxAzwifgTsqJj9duCr+fOvAu+ocl1j6vJHypqZTXkMfGlEbM6fbwGWjraipGsk9Urq7evrm+LhhvNHypqZVeEiZkQEEGMsXx0RqyJiVU9Pz3QPB0C3e+BmZlMO8K2SlgHk/26rXknjcw/czGzqAX4ncEX+/ArgO9UpZ2I8Bm5mNrHbCG8Dfgq8WNJzkq4CPg28SdKTwBvz6VnT3e4euJlZabwVIuKyURa9ocq1TFhXa4lDRwfpHxik1Jzce5HMzKoiyfTrasveTu+vVTOzRpZkgB/7QCt/qYOZNbA0A7zdnwluZpZkgPtbeczMEg3wef5SBzOzNAO8Ox9C2XPoaI0rMTOrnSQDfEFHKwC7DzrAzaxxpRngnS0A7DrgADezxpVkgLe3NNNWanIP3MwaWpIBDlkvfNeBI7Uuw8ysZpIN8IWdrR5CMbOGlmyAz+9oYZeHUMysgSUb4As6W9jtHriZNbB0A7yjlV0HPQZuZo0r3QDvbPEYuJk1tGQDfH5nC4f7Bzl4xB8pa2aNKdkAL78b08MoZtao0g1wvxvTzBpcugHe4QA3s8aWbIDPz3vguz2EYmYNKtkAX9CZj4G7B25mDSrZAF9YHgP3uzHNrEElG+AdLc20Nje5B25mDSvZAJfE/M4Wj4GbWcNKNsAhuxPFPXAza1RpB3hnCzv9meBm1qCSDvBFXa38ep8D3MwaU9IBfmJ3O9v2Hq51GWZmNZF0gPd0t7H74FEO9/sDrcys8SQd4Cd2twHQ5164mTWgtAP8BAe4mTWupAO8Z147gMfBzawhJR3g5R64A9zMGlHSAb64qxXJQyhm1piSDvBScxOLu1rp23uo1qWYmc26pAMcoKe7nW173AM3s8ZTBwHeRt8+B7iZNZ7kA/zE7jb3wM2sIdVFgG/fd5jBwah1KWZms6o0nY0lbQT2AgNAf0SsqkZRk9HT3Ub/YLDzwBEWz2ub7cObmdXMtAI897qI2F6F/UzJid1Db+ZxgJtZI0l+COWk+Vlob959sMaVmJnNrukGeADfk7RW0jUjrSDpGkm9knr7+vqmebjjLV/UCcCmHQ5wM2ss0w3w346IVwBvBT4g6bWVK0TE6ohYFRGrenp6pnm44/XMa6O9pYlNOw5Ufd9mZnPZtAI8Ip7P/90GfBu4oBpFTYYkli/s5FkHuJk1mCkHuKQuSd3l58CbgceqVdhknLqok007PYRiZo1lOnehLAW+Lam8n69HxHerUtUkLV/UyQO/2kFEkNdjZlb3phzgEfE0cG4Va5my5Ys62Xe4n10HjrKwq7XW5ZiZzYrkbyMEWL6wA8Dj4GbWUOoiwE9dnN9KuNMBbmaNoy4CfPnCLMDdAzezRlIXAd7VVmJxV6vfzGNmDaUuAhxgxeJOfrV9X63LMDObNXUT4C8+qZv1W/YS4Y+VNbPGUDcBftZJJ7DzwFF/Q72ZNYw6CvBuANZt3lPjSszMZkcdBfgJAKzfsrfGlZiZzY66CfD5nS0sm9/OLx3gZtYg6ibAIbuQ6SEUM2sUdRXgZ510Ak/17ePowGCtSzEzm3F1FeAvWdbN0YHgia0eRjGz+ldXAb5q5SIAfvarHTWuxMxs5tVVgJ+8oIOTF3Tw4EYHuJnVv7oKcIALT1vEg/mXO5iZ1bO6C/ALTlvE9n1HeKpvf61LMTObUXUZ4AAPehzczOpc3QX4aUu66Olu48cbtte6FDOzGVV3AS6JN529lPvWb+PgkYFal2NmNmPqLsABLn3pMg4cGeCHT2yrdSlmZjOmLgP8otMXsbCzhbsf3VLrUszMZkxdBnipuYm3nHMSa9Zt9TCKmdWtugxwgHe94hT2HxngWw89V+tSzMxmRN0G+G+tXMjLT5nPzff/isFBv6nHzOpP3Qa4JK5+zek8vX0/31+3tdblmJlVXd0GOMBbX3oSKxZ38pl71nOk3x8xa2b1pa4DvNTcxPW/ew4btu3jK/c/XetyzMyqqq4DHOB1Z53IW85Zyue//ySPPre71uWYmVVN3Qc4wA3vfBlL5rVx9dd62bL7UK3LMTOrioYI8CXz2vjKFavYe+go7/mHn/BU375al2RmNm0NEeAAL1l2Al+/+iIOHhngHV/4Mbc9+KxvLzSzpDVMgAOcu3wB//qBV3POySfwkTse5dK//3f+Ze1z7Dl0tNalmZlNmmbzm2tWrVoVvb29s3a80QwOBt/5xfN88b6n2LBtH63NTbzqjMW8csVCXnbKfFYu7uI3FrTTVmqudalmZkhaGxGrKueXalFMrTU1iXeefwpvP/dkfr5pF//v0c384Ik+fvRkH+XfZ1I2dr6go4UTOlrobi9xQnsL7S1NtDSXH6KUP29tFs1NTUjQJBBCyt5QJPJ5qpynYeujfB7Q1JTPy2spK29bnl9cpzifEedXbJtPlI9d3kr5AjF8HRXWYZT5x7adRD0jHYNR5k/7nEds0yqdc8W2DKt5cvUcez7S8sK+itPF9VTcwOpWQwZ4WVOTeOWKhbxyxUL+Cth76CjrNu9l044DPLfzIC/sOsieQ0fZe6ifHfuPsHH7fg4dHaR/cJAj/YP0DwZHBwY5OuCxdJv7hv0yYnjIq2KdbN7wDUb8RTHJXzAj72v4PkZcNkJ9jHnsinMYYf8j/ZIrdhSG9jm9cyyv86l3vuzYN4ZVS0MHeKXu9hYuOG3RpBs5IugfDPoHgiCIgMEIAojIlo84j3xeDP1LPq98fbW8Hvl25FMRDJsfxfnlbRna59D2w+dHfoyh58fWrlhn5HqOTY1VD8dvS1TUMcp6Ez7nwraMts4o5zx0CqOcc8V8Rmiv49tlAvWMMr9S5T4KJQ//+VbsY9iuiuc7znYjrRccv3C0c5zMcYa2G17fSMceq77i3OJrarRjj3WcEU51+Dket90Y51FY2NVW/SFZB3gVSKKlWbR4yNzMZlFD3YViZlZPphXgki6RtF7SBknXVasoMzMb35QDXFIz8EXgrcDZwGWSzq5WYWZmNrbp9MAvADZExNMRcQT4Z+Dt1SnLzMzGM50APxnYVJh+Lp9nZmazYMYvYkq6RlKvpN6+vr6ZPpyZWcOYToA/DywvTJ+SzxsmIlZHxKqIWNXT0zONw5mZWdF0AvxnwJmSTpPUCvw+cGd1yjIzs/FM68OsJF0KfA5oBm6OiBvGWb8PeGaKh1sCbJ/ito3CbTQ+t9HEuJ3GN5tttCIijhvCmNVPI5wOSb0jfRqXDXEbjc9tNDFup/HNhTbyOzHNzBLlADczS1RKAb661gUkwG00PrfRxLidxlfzNkpmDNzMzIZLqQduZmYFDnAzs0QlEeD+2NqMpJslbZP0WGHeIkn3Snoy/3dhPl+S/j5vs0ckvaJ2lc8eScsl3SfpPyQ9LunafL7bKSepXdKDkn6Rt9En8vmnSXogb4tv5G/QQ1JbPr0hX76ylvXPJknNkn4u6a58ek610ZwPcH9s7TC3AJdUzLsOWBMRZwJr8mnI2uvM/HENcOMs1Vhr/cCHIuJs4CLgA/nrxe005DDw+og4FzgPuETSRcDfAJ+NiBcBO4Gr8vWvAnbm8z+br9corgXWFabnVhtl39c4dx/Aq4B7CtMfAT5S67pq2B4rgccK0+uBZfnzZcD6/PmXgctGWq+RHsB3gDe5nUZtn07gIeBCsncVlvL5x/7fAfcAr8qfl/L1VOvaZ6FtTiH7Zf964C6y7ymeU20053vg+GNrx7M0Ijbnz7cAS/PnDd9u+Z+x5wMP4HYaJh8aeBjYBtwLPAXsioj+fJViOxxro3z5bmDx7FZcE58DPgwM5tOLmWNtlEKA2wRF9uvf94UCkuYB3wI+GBF7isvcThARAxFxHlkv8wLgrBqXNKdIehuwLSLW1rqWsaQQ4BP62NoGtlXSMoD83235/IZtN0ktZOF9a0Tckc92O40gInYB95ENByyQVMoXFdvhWBvly+cDv57lUmfbq4Hfk7SR7NvGXg98njnWRikEuD+2dmx3Alfkz68gG/Mtz/+j/C6Li4DdhSGEuiVJwE3Auoj4u8Iit1NOUo+kBfnzDrJrBOvIgvw9+WqVbVRuu/cA/5b/FVO3IuIjEXFKRKwky5x/i4jLmWttVOsLBRO8mHAp8ATZON3/rHU9NWyH24DNwFGy8beryMbZ1gBPAt8HFuXriuzunaeAR4FVta5/ltrot8mGRx4BHs4fl7qdhrXRy4Gf5230GPCxfP7pwIPABuB2oC2f355Pb8iXn17rc5jl9roYuGsutpHfSm9mlqgUhlDMzGwEDnAzs0Q5wM3MEuUANzNLlAPczCxRDnAzs0Q5wM3MEvX/Aeox3HWHaxApAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZQWB-cjqa-D"
      },
      "source": [
        "---\n",
        "**Points:** $0.0$ of $0.25$\n",
        "\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "vHhFhjZNqa-D"
      },
      "source": [
        "#### 4.3.10. Evaluation on Test set\n",
        "**Task 15:** 0.25 points <br >\n",
        "Evaluate your model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwGzjx-4qa-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08723078-2ba5-413e-a962-56032208fbe3"
      },
      "source": [
        "# test your model on X_test with the weight vector that you found above\n",
        "# this will be the generalization error of our model.\n",
        "\n",
        "X_test = test_data[:, :-1]\n",
        "Y_test = test_data[:, -1:]\n",
        "w_best  = train_params_results[order_best][lambda_best][lr_best]['w']\n",
        "\n",
        "X_test_order, _ = prepare_data_matrix(X_test, w_best, order_best)\n",
        "preds = get_prediction(X_test_order.float(), w_best.float())\n",
        "test_mse = compute_mse(preds, Y_test)\n",
        "print(test_mse)\n",
        "\n",
        "# TODO: implement"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4883, dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePtrUoyEqa-D"
      },
      "source": [
        "---\n",
        "**Points:** $0.0$ of $0.25$\n",
        "\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogQrVkZaqa-E"
      },
      "source": [
        "#### 4.3.11. Results\n",
        "**Task 16:** 0.25 points <br >\n",
        "Report the MSE value on the test data. Which hyperparameter combination turned out to be the best? In your understanding, why do you think such a combination turned out to be the best for this task?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1VYb4LVUbnG"
      },
      "source": [
        "Answer: The MSE value on the test data is found to be 0.4883 with the order of 1 and corresponding lambda and learning rate are 0.1 and 1e-05 respectively. \n",
        "\n",
        "The learning rate of lr=1e-5 is already very small and going below this value will make the convergence almost negligible. Similar is the case for lambda where 0.1 is already high enough, increasing this value further would again make the convergence almost negligible.\n",
        "\n",
        "As for linear regression fitting the dataset better than the higher order polynomial regression, probably the dataset has a few outliers and higher order polynomial regression is massively effected by such outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FaYWUcTqa-E"
      },
      "source": [
        "---\n",
        "**Points:** $0.0$ of $0.25$\n",
        "\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "kVGh8eDvqa-E"
      },
      "source": [
        "## Submission instructions\n",
        "You should provide a single Jupyter notebook (.ipynb file) as the solution. Write the names and student ids of your team members below. **Make sure to submit only 1 solution to only 1 tutor.** Also, follow the naming conventions mentioned in the assignment sheet for naming the jupyter notebook.\n",
        "\n",
        "- Harsh Agarwal, 7024725\n",
        "- Zurana Mehrin Ruhi, 7023892"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYpwK_Xpqa-E"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIFqLWUFqa-E"
      },
      "source": [
        "## Points: 0.0 of 8.0 points"
      ]
    }
  ]
}