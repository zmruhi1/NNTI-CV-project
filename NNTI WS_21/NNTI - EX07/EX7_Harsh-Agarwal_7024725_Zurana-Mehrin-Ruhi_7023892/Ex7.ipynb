{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VGGjeMOyLhv"
   },
   "source": [
    "# Team members\n",
    "Name: Harsh Agarwal  \n",
    "Matrikelnummer: 7024725  \n",
    "email: haag00001@stud.uni-saarland.de\n",
    "\n",
    "Name: Zurana Mehrin Ruhi  \n",
    "Matrikelnummer: 7023892  \n",
    "email: zuru00001@stud.uni-saarland.de\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5QDWCNfyLhy"
   },
   "source": [
    "# 7.4 Dropout and Regularization\n",
    "## DO NOT EDIT THIS FILE FROM HERE ONWARDS\n",
    "\n",
    "Bishop et al proposed a simple idea to regularize L2 loss function by injecting noise in the data.\n",
    "Then, in 2014, Srivastava et al. [Srivastava et al., 2014] developed a clever idea for how to apply Bishop’s idea to the internal layers of a network, too. Namely, they proposed to inject noise into each layer of the network before calculating the subsequent layer during training. They realized that when training a deep network with many layers, injecting noise enforces smoothness just on the input-output mapping.\n",
    "\n",
    "Their idea, called dropout, involves injecting noise while computing each internal layer during forward propagation, and it has become a standard technique for training neural networks. The method is called dropout because we literally drop out some neurons during training. Throughout training, on each iteration, standard dropout consists of zeroing out some fraction of the nodes in each layer before calculating the subsequent layer.\n",
    "\n",
    "The key challenge then is how to inject this noise. One idea is to inject the noise in an unbiased manner so that the expected value of each layer—while fixing the others—equals to the value it would have taken absent noise.\n",
    "\n",
    "In Bishop’s work, he added Gaussian noise to the inputs to a linear model. At each training iteration, he added noise sampled from a distribution with mean zero  $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$  to the input  $x$, yielding a perturbed point  $x'= x+\\epsilon$ . In expectation,  E[x'] = x .\n",
    "\n",
    "In inverted dropout regularization, one debiases each layer by normalizing by the fraction of nodes that were retained (not dropped out). In other words, with dropout probability  p , each intermediate activation h is replaced by a random variable h' as follows:\n",
    "\n",
    "h'  = 0 with probability p  \n",
    "h'  = h / (1-p) otherwise  \n",
    "\n",
    "By design, the expectation remains unchanged, i.e.,  𝐸[ℎ′]=ℎ.\n",
    "Also read the interesting dropout paper in the link below\n",
    "\n",
    "https://jmlr.org/papers/v15/srivastava14a.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gm2jdk3LyLhz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activations import ReLU, LeakyReLU, Tanh, Softmax, Sigmoid\n",
    "from losses import CrossEntropy, MSELoss\n",
    "from layers import Linear\n",
    "from layers import L2regularization, Dropout\n",
    "from model import Model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQ0W6cP4yLh1"
   },
   "source": [
    "## 7.4.1 Dropout (1.5 points)\n",
    "In this exercise we are going to implement inverted dropout. \n",
    "We implement dropout as a layer wrapper where Dropout class takes two arguments\n",
    "Dropout (layer, probability). Although dropout can be applied to several types\n",
    "of layers, we only apply it to linear layers in this exercise. Use inverted dropout\n",
    "in this exercise. Implement dropout in ./layers/Dropout.py which transforms the input by setting randomly chosen activations to 0\n",
    "\n",
    "### Implement the forward pass in ./layers/Dropout.py (1 point)\n",
    "You have to implement the \\__call\\__() function which applies dropout to \n",
    "the linear layer.\n",
    "\n",
    "### Implement the backward pass in ./layers/Dropout.py (0.5 point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qSDSstDfyLh2",
    "outputId": "2c805da2-b60b-45ce-dc49-65a49348afb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0169578738329816\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "layer1 = Linear(1000, 100)\n",
    "activation1 = ReLU()\n",
    "layer2 = Dropout(Linear(100, 10), p=0.5)\n",
    "activation2 = ReLU()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "x = np.random.randn(2, 1000)\n",
    "y_true = np.zeros((2, 10,))\n",
    "y_true[0, 4] = 1\n",
    "y_true[1, 1] = 1\n",
    "m = Model([layer1, activation1, layer2, activation2])\n",
    "out = m.forward(x)\n",
    "\n",
    "# numpy seed is fixed so you should get the same value after each run\n",
    "print(loss(out, y_true)) # = 2.15028 with tolerance 5e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcWkss4FyLh3"
   },
   "source": [
    "## 7.4.2 L2 Regularization (1.5 point)\n",
    "In this exercise we are going to implement a layer wrapper to Linear layer\n",
    "that applies L2 regularization to the layer. We add the squared norm of the weights\n",
    "of one layer to the loss function during the forward pass. We modify the backward pass\n",
    "of the linear layer to incorporate the regularization term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gR-8a8B5yLh3",
    "outputId": "9e987197-476e-438c-b833-a52771b52a76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9887497161910503\n"
     ]
    }
   ],
   "source": [
    "# Implement a simple two layer model in our framework\n",
    "# with L2 regularization loss added in the end\n",
    "\n",
    "from model import Model\n",
    "np.random.seed(123)\n",
    "\n",
    "reg_coeff = 0.01\n",
    "\n",
    "layer1 = Linear(1000, 100)\n",
    "activation1 = ReLU()\n",
    "layer2 = L2regularization(Linear(100, 10), coefficient=reg_coeff)\n",
    "activation2 = ReLU()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "x = np.random.randn(2, 1000)\n",
    "y_true = np.zeros((2, 10,))\n",
    "y_true[0, 4] = 1\n",
    "y_true[1, 1] = 1\n",
    "m = Model([layer1, activation1, layer2, activation2])\n",
    "out = m.forward(x)\n",
    "\n",
    "regularization = reg_coeff * np.sum(np.square(layer2.weights))\n",
    "\n",
    "# Print the regularized loss value\n",
    "print(loss(out, y_true) + regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1_UbycO-yLh4",
    "outputId": "597246c3-471b-4b68-8228-03c7d02c4248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9887497425079346\n"
     ]
    }
   ],
   "source": [
    "# Create a similar model in pytorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1000, 100, bias=True)\n",
    "        self.layer2 = nn.Linear(100, 10, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return x\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = Net()\n",
    "\n",
    "# initialize it to the same weights\n",
    "with torch.no_grad():\n",
    "    net.layer1.weight.copy_(torch.tensor(layer1.weights).t())\n",
    "    net.layer1.bias.copy_(torch.tensor(layer1.bias[0,:]))\n",
    "    net.layer2.weight.copy_(torch.tensor(layer2.weights).t())\n",
    "    net.layer2.bias.copy_(torch.tensor(layer2.bias[0,:]))\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0)\n",
    "\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "out = net(x_torch)\n",
    "y_true_torch = torch.tensor(y_true, dtype=torch.float32)\n",
    "loss_torch = criterion(out, y_true_torch) + (reg_coeff * torch.sum(torch.square( net.layer2.weight)))\n",
    "print(loss_torch.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWJQGkVryLh4",
    "outputId": "5b4900d9-cf05-4837-c2db-bb9634b10e68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8874745546591278\n"
     ]
    }
   ],
   "source": [
    "# Calculate the gradients of our network in our toy framework\n",
    "grads = m.backward(loss.grad())\n",
    "m.update_parameters(grads, 0.001)\n",
    "out = m.forward(x)\n",
    "regularization = reg_coeff * np.sum(np.square(layer2.weights))\n",
    "# Print the loss with the updated gradients\n",
    "model_loss_ours = loss(out, y_true) + regularization\n",
    "print(model_loss_ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xOAIM5dayLh5",
    "outputId": "1d2d60db-18df-43b2-d921-034ca77ad5c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8871, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the gradients of our pytorch model\n",
    "loss_torch.backward()\n",
    "optimizer.step()\n",
    "# Print the loss with the updated gradients\n",
    "out = net(x_torch)\n",
    "model_loss_pt = criterion(out, y_true_torch).item() + (reg_coeff * torch.sum(torch.square( net.layer2.weight)))\n",
    "print(model_loss_pt)\n",
    "\n",
    "# Similar to A6 we compare the loss of pytorch model and our model\n",
    "np.allclose(model_loss_ours, model_loss_pt.detach(), atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNNOu91nyLh5"
   },
   "source": [
    "The difference between the loss with the updated gradients of our model and the pytorch model\n",
    "should not be more that 0.001"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ex7.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "29752ed4bfb91d83500200df72d510ca9c3a3b0b0b2c5a1f25d056f0c4d3fa53"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
