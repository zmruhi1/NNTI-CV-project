{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team members\n",
    "Name: Harsh Agarwal\n",
    "Matrikelnummer: 7024725\n",
    "email: haag00001@stud.uni-saarland.de\n",
    "\n",
    "Name: Zurana Mehrin Ruhi\n",
    "Matrikelnummer: 7023892\n",
    "email: zuru00001@stud.uni-saarland.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5.4 (4 points)\n",
    "### **Do not edit this notebook**\n",
    "Implement the __call__() method in every activation function. You don't have to implement backward() anywhere for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activations import ReLU, LeakyReLU, Tanh, Softmax, Sigmoid\n",
    "from losses import CrossEntropy, MSELoss\n",
    "from layers import Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex5.4 a Implement ReLU() activation function (0.5 point)\n",
    "Implement the __call__ function in ./activations/ReLU.py which takes a tensor x as input and applies the ReLU activation function on x. \n",
    "\n",
    "f(x) = max(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0. , 0.5, 0.9, 0. , 0. ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu = ReLU()\n",
    "x = np.array([0.1, -0.3, 0.5, 0.9, 0, -1.0])\n",
    "relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex5.4 b Implement LeakyReLU() activation function (0.5 point)\n",
    "Implement the __call__ function in ./activations/LeakyReLU.py which takes a tensor x as input and applies the LeakyReLU activation function on x.\n",
    "\n",
    "$f(x) = \\alpha x \\text{ if } x < 0$    \n",
    "$f(x) =   x \\text{ if } x \\geq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu = LeakyReLU(alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1  , -0.003,  0.5  ,  0.9  ,  0.   , -0.01 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_relu = LeakyReLU(alpha=0.01)\n",
    "x = np.array([0.1, -0.3, 0.5, 0.9, 0, -1.0])\n",
    "leaky_relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex5.4 c Implement Tanh() activation function (0.5 point)\n",
    "Implement the __call__ function in ./activations/Tanh.py which takes a tensor x as input and applies the Softmax activation function on x.  \n",
    "$f(x) = \\tanh(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09966799, -0.29131261,  0.46211716,  0.71629787,  0.        ,\n",
       "       -0.76159416])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh = Tanh()\n",
    "tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex5.4 d Implement Softmax() activation function (0.5 point)\n",
    "Implement the __call__ function in ./activations/Softmax.py which takes a tensor x as input and applies the Softmax activation function on x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15093442, 0.10117436, 0.22516769, 0.33591072, 0.13657111,\n",
       "       0.0502417 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = Softmax()\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex5.4 e Implement Sigmoid() activation function (0.5 point)\n",
    "Implement the __call__ function in ./activations/Sigmoid.py which takes a tensor x as input and applies the Sigmoid activation function on x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52497919, 0.42555748, 0.62245933, 0.7109495 , 0.5       ,\n",
       "       0.26894142])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid = Sigmoid()\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex5.4 f Implement CrossEntropy Loss (0.5 point)\n",
    "mplement the __call__ function in ./losses/CrossEntropy.py which takes a predictions and true values as agruments and finds the cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47080426832665845"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_loss = CrossEntropy()\n",
    "predictions = np.array([[0.4,0.35,0.71,0.60],\n",
    "                        [0.01,0.01,0.01,0.65]])\n",
    "targets = np.array([[0,0,0,1],\n",
    "                  [0,0,0,1]])\n",
    "ce_loss(predictions, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex5.4 g Implement MSE Loss (0.5 point)\n",
    "Implement the __call__ function in ./losses/MSELoss.py which takes a predictions and true values as agruments and finds the mean squared error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49000000000000005"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss = MSELoss()\n",
    "y = np.array([0.6, 0.3, 0.5, 0.1, 1.3, -1.0])\n",
    "mse_loss(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex5.4 h Implement Linear() layer (0.5 point)\n",
    "Implement the __call__ function in ./layers/Linear.py which is y = Wx + b where b is the bias variable. We will do a simple forward pass of with two linear layers and one activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.random.randn(20, 100)\n",
    "layer1 = Linear(100, 20)\n",
    "layer2 = Linear(20, 10)\n",
    "\n",
    "h1 = layer1(input_data)\n",
    "z1 = relu(h1)\n",
    "h2 = layer2(z1)\n",
    "z2 = softmax(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
